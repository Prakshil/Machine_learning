{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz1zpmVXhMBO",
        "outputId": "2642c0ce-31e8-47e4-c160-382f318f7d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         Author              Rating\n",
            "0                                      PR Yadav  4.4 out of 5 stars\n",
            "1                                           N/A  4.5 out of 5 stars\n",
            "2                                   James Allen  4.4 out of 5 stars\n",
            "3                                     DC Pandey  4.2 out of 5 stars\n",
            "4                                           N/A  4.4 out of 5 stars\n",
            "5                                           N/A  4.6 out of 5 stars\n",
            "6                              Shaurrya Singhvi                 N/A\n",
            "7   Sachin Kumarr Laxman Prasad , Anjali Sharma  4.5 out of 5 stars\n",
            "8                        ALLEN Expert Faculties  4.3 out of 5 stars\n",
            "9                                           N/A  4.5 out of 5 stars\n",
            "10                                 Preeti Gupta  4.4 out of 5 stars\n",
            "11                                          N/A  4.2 out of 5 stars\n",
            "12                       ALLEN Expert Faculties  4.1 out of 5 stars\n",
            "13                           Wonder House Books  4.6 out of 5 stars\n",
            "14                                Upinder Singh  4.5 out of 5 stars\n",
            "15                                  Kriti Arora  4.5 out of 5 stars\n",
            "16                       ALLEN Expert Faculties  4.3 out of 5 stars\n",
            "17                                   R.K. Gupta  4.5 out of 5 stars\n",
            "18                                          N/A  4.4 out of 5 stars\n",
            "19                                 Sweta Adatia  4.0 out of 5 stars\n",
            "20                           Subhadra Sen Gupta  4.5 out of 5 stars\n",
            "21                           Wonder House Books  4.7 out of 5 stars\n",
            "22                                 Aman Kharwal  4.3 out of 5 stars\n",
            "23                                          N/A  4.3 out of 5 stars\n",
            "24                                          N/A                 N/A\n",
            "25                                  Kriti Arora  4.4 out of 5 stars\n",
            "26                                 Mike Chapple  4.6 out of 5 stars\n",
            "27                                 Naresh Kumar  4.1 out of 5 stars\n",
            "28                       ALLEN Expert Faculties  4.1 out of 5 stars\n",
            "29                                          N/A  3.3 out of 5 stars\n",
            "Saved 30 reviews to amazon_reviews_sample.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://www.amazon.in/gp/bestsellers/books/4149461031/ref=zg_bs_pg_{}?ie=UTF8&pg={}\"\n",
        "\n",
        "headers = {\n",
        "\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "book_list = []\n",
        "\n",
        "for page in range(1,6):\n",
        "\n",
        "    # construct the URL for the current page\n",
        "\n",
        "    url=base_url.format(page, page)\n",
        "\n",
        "\n",
        "\n",
        "    # send a GET request to the URL\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "\n",
        "\n",
        "    # parse the HTML content of the page\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "\n",
        "\n",
        "    # find all book entries on the page\n",
        "\n",
        "    books = soup.find_all(\"div\", class_=\"zg-grid-general-faceout\")\n",
        "\n",
        "\n",
        "\n",
        "    # iterate through each book entry and extract details\n",
        "\n",
        "    for book in books:\n",
        "\n",
        "        if len(book_list) < 50:  # stop once we've collected 50 books\n",
        "\n",
        "            author = book.find(\"a\", class_=\"a-size-small a-link-child\").get_text(strip=True) if book.find(\"a\", class_=\"a-size-small a-link-child\") else \"N/A\"\n",
        "\n",
        "            rating = book.find(\"span\", class_=\"a-icon-alt\").get_text(strip=True) if book.find(\"span\", class_=\"a-icon-alt\") else \"N/A\"\n",
        "\n",
        "\n",
        "\n",
        "            # append the extracted data\n",
        "\n",
        "            book_list.append({\"Author\": author,\n",
        "\n",
        "                            \"Rating\": rating})\n",
        "\n",
        "        else:\n",
        "\n",
        "            break\n",
        "\n",
        "df=pd.DataFrame(book_list)\n",
        "\n",
        "print(df)\n",
        "\n",
        "df.to_csv(\"amazon_top_50_books_authors_ratings.csv\", index=False)\n",
        "\n",
        "print(f\"Saved {len(df)} reviews to amazon_reviews_sample.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLOv1pJviQ39",
        "outputId": "24e03de6-58e7-4d62-dd6c-7558274b4706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Failed to fetch page 1: 529\n",
            "Fetching page 2...\n",
            "Failed to fetch page 2: 529\n",
            "Fetching page 3...\n",
            "Failed to fetch page 3: 529\n",
            "Saved 0 reviews to flipkart_reviews_sample.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "PRODUCT_SLUG = \"itmfy2jxm4h87ptd\"  # Slug from the Flipkart review URL\n",
        "\n",
        "PRODUCT_PID = \"SUGFY2JXQQKRYFBP\"  # PID from the Flipkart review URL\n",
        "\n",
        "HEADERS = {\n",
        "\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "def get_reviews(page_no):\n",
        "\n",
        "    # Flipkart review URL structure\n",
        "\n",
        "    url = f\"https://www.flipkart.com/flipkart/product-reviews/{PRODUCT_SLUG}?pid={PRODUCT_PID}&page={page_no}\"\n",
        "\n",
        "\n",
        "\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "\n",
        "        print(f\"Failed to fetch page {page_no}: {response.status_code}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "\n",
        "\n",
        "    for review_block in soup.find_all(\"div\", class_=\"_2MIQdG\"):\n",
        "\n",
        "        review = {}\n",
        "\n",
        "        # Review Title\n",
        "\n",
        "        review[\"review_title\"] = review_block.find(\"p\", class_=\"_2-N8zT\").text.strip() if review_block.find(\"p\", class_=\"_2-N8zT\") else \"N/A\"\n",
        "\n",
        "\n",
        "\n",
        "        rating_element = review_block.find(\"div\", class_=\"_3LWZlK\")\n",
        "\n",
        "        review[\"rating\"] = rating_element.text.strip() if rating_element else \"N/A\"\n",
        "\n",
        "        # Review Body\n",
        "\n",
        "        review[\"review_body\"] = review_block.find(\"div\", class_=\"t-ZTKy\").text.strip() if review_block.find(\"div\", class_=\"t-ZTKy\") else \"N/A\"\n",
        "\n",
        "        # Author and Date are often in the same parent element\n",
        "\n",
        "        author_date_element = review_block.find(\"p\", class_=\"_2sc7ZR\")\n",
        "\n",
        "        if author_date_element:\n",
        "\n",
        "            spans = author_date_element.find_all(\"span\")\n",
        "\n",
        "            if len(spans) >= 1:\n",
        "\n",
        "                review[\"author\"] = spans[0].text.replace(\"Verified Buyer\", \"\").strip() if \"Verified Buyer\" in spans[0].text else spans[0].text.strip()\n",
        "\n",
        "            else:\n",
        "\n",
        "                review[\"author\"] = \"N/A\"\n",
        "\n",
        "\n",
        "            date_text = \"N/A\"\n",
        "\n",
        "            for span in spans:\n",
        "\n",
        "                if \"Reviewed on\" in span.text:\n",
        "\n",
        "                    date_text = span.text.replace(\"Reviewed on\", \"\").strip()\n",
        "\n",
        "                    break\n",
        "\n",
        "                elif len(spans) > 1 and spans.index(span) == 1: # If there's a second span, assume it's date\n",
        "\n",
        "                    date_text = span.text.strip()\n",
        "\n",
        "                    break\n",
        "\n",
        "            review[\"date\"] = date_text\n",
        "\n",
        "        else:\n",
        "\n",
        "            review[\"author\"] = \"N/A\"\n",
        "\n",
        "            review[\"date\"] = \"N/A\"\n",
        "\n",
        "\n",
        "\n",
        "        reviews.append(review)\n",
        "\n",
        "    return reviews\n",
        "\n",
        "\n",
        "def scrape_flipkart_reviews(pages=3):\n",
        "\n",
        "    all_reviews = []\n",
        "\n",
        "    for p in range(1, pages+1):\n",
        "\n",
        "        print(f\"Fetching page {p}...\")\n",
        "\n",
        "        reviews = get_reviews(p)\n",
        "\n",
        "        all_reviews.extend(reviews)\n",
        "\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "    return all_reviews\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    reviews_data = scrape_flipkart_reviews(pages=3)\n",
        "\n",
        "    df = pd.DataFrame(reviews_data)\n",
        "\n",
        "    df.to_csv(\"flipkart_reviews_sample.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"Saved {len(df)} reviews to flipkart_reviews_sample.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}